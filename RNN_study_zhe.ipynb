{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN study zhe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdv0fMlhK9m2+ohN7PtNhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egorzhukov-it/medium_study/blob/main/RNN_study_zhe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtTYRYxonP4c"
      },
      "source": [
        "Тренировочный блокнот для изучения RNN по статье:\r\n",
        "\r\n",
        "Medium: https://medium.com/@annikabrundyn1/the-beginners-guide-to-recurrent-neural-networks-and-text-generation-44a70c34067f\r\n",
        "\r\n",
        "Github:\r\n",
        "https://github.com/annikabrundyn/rnn_text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzaY05XBnO8B"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhW2yqAepfly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4d69fe-6b68-4bed-d177-7be78a37fc2f"
      },
      "source": [
        "# получаем файл из github\r\n",
        "!git clone \"https://github.com/egorzhukov-it/medium_study.git\"\r\n",
        "\r\n",
        "text = (open(\"medium_study/wonderland.txt\").read())\r\n",
        "text = text.lower()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'medium_study'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbKMIderusNH",
        "outputId": "8c98e436-8821-4312-a8d4-f4fe999a189b"
      },
      "source": [
        "# полчаем словарь символов\r\n",
        "\r\n",
        "characters = sorted(list(set(text)))\r\n",
        "\r\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\r\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\r\n",
        "\r\n",
        "vocab_size = len(characters)\r\n",
        "print('Number of unique characters: ', vocab_size)\r\n",
        "print(characters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters:  42\n",
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PcidA_ZKe1W",
        "outputId": "c207f429-1dae-4cff-8123-96977465ae8d"
      },
      "source": [
        "X = []   # extracted sequences\r\n",
        "Y = []   # the target: follow up character for each sequence in X\r\n",
        "length = len(text)\r\n",
        "seq_length = 100\r\n",
        "\r\n",
        "for i in range(0, length - seq_length, 1):\r\n",
        "    sequence = text[i:i + seq_length]\r\n",
        "    label = text[i + seq_length]\r\n",
        "    X.append([char_to_n[char] for char in sequence])\r\n",
        "    Y.append(char_to_n[label])\r\n",
        "    \r\n",
        "print('Number of extracted sequences:', len(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of extracted sequences: 143452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwJ8gp0WMCR2"
      },
      "source": [
        "# модифицируем входные данные до тензора размерностью (batch_size, sequence_size, feture_size)\r\n",
        "\r\n",
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\r\n",
        "X_modified = X_modified / float(len(characters))\r\n",
        "Y_modified = np_utils.to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G8hvEmPMRlH"
      },
      "source": [
        "# архитектура сети\r\n",
        "\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True, dropout=0.2),\r\n",
        "    tf.keras.layers.LSTM(700, return_sequences=True, dropout=0.2),\r\n",
        "    tf.keras.layers.LSTM(700, dropout=0.2),\r\n",
        "    tf.keras.layers.Dense(Y_modified.shape[1], activation=tf.nn.softmax),\r\n",
        "])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXcVPDhSOBs0",
        "outputId": "bb221953-2e4b-4801-e25c-4444deb8379e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 700)          1965600   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 700)          3922800   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 700)               3922800   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 42)                29442     \n",
            "=================================================================\n",
            "Total params: 9,840,642\n",
            "Trainable params: 9,840,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNTp62aXPgvR"
      },
      "source": [
        "# # load the network weights saved in the folder model_weights\r\n",
        "# filename = \"medium_study/baseline-improvement-06-0.9927.hdf5\"\r\n",
        "# model.load_weights(filename)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "# define how model checkpoints are saved\r\n",
        "filepath = \"model_weights/gigantic-improvement-ctd20-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mgRtsVyXIb-",
        "outputId": "695e8808-b130-4316-a041-5dbbb48ef975"
      },
      "source": [
        "model.fit(X_modified, Y_modified, epochs=10, batch_size=128, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1121/1121 [==============================] - 322s 278ms/step - loss: 3.0321\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.94795, saving model to model_weights/gigantic-improvement-ctd20-01-2.9480.hdf5\n",
            "Epoch 2/10\n",
            "1121/1121 [==============================] - 331s 295ms/step - loss: 2.7624\n",
            "\n",
            "Epoch 00002: loss improved from 2.94795 to 2.70172, saving model to model_weights/gigantic-improvement-ctd20-02-2.7017.hdf5\n",
            "Epoch 3/10\n",
            "1121/1121 [==============================] - 332s 296ms/step - loss: 2.5267\n",
            "\n",
            "Epoch 00003: loss improved from 2.70172 to 2.48213, saving model to model_weights/gigantic-improvement-ctd20-03-2.4821.hdf5\n",
            "Epoch 4/10\n",
            "1121/1121 [==============================] - 333s 297ms/step - loss: 2.3603\n",
            "\n",
            "Epoch 00004: loss improved from 2.48213 to 2.32818, saving model to model_weights/gigantic-improvement-ctd20-04-2.3282.hdf5\n",
            "Epoch 5/10\n",
            "1121/1121 [==============================] - 332s 296ms/step - loss: 2.2294\n",
            "\n",
            "Epoch 00005: loss improved from 2.32818 to 2.20672, saving model to model_weights/gigantic-improvement-ctd20-05-2.2067.hdf5\n",
            "Epoch 6/10\n",
            "1121/1121 [==============================] - 334s 298ms/step - loss: 2.1318\n",
            "\n",
            "Epoch 00006: loss improved from 2.20672 to 2.11618, saving model to model_weights/gigantic-improvement-ctd20-06-2.1162.hdf5\n",
            "Epoch 7/10\n",
            "1121/1121 [==============================] - 334s 298ms/step - loss: 2.0382\n",
            "\n",
            "Epoch 00007: loss improved from 2.11618 to 2.03471, saving model to model_weights/gigantic-improvement-ctd20-07-2.0347.hdf5\n",
            "Epoch 8/10\n",
            "1121/1121 [==============================] - 334s 298ms/step - loss: 1.9784\n",
            "\n",
            "Epoch 00008: loss improved from 2.03471 to 1.96515, saving model to model_weights/gigantic-improvement-ctd20-08-1.9652.hdf5\n",
            "Epoch 9/10\n",
            "1121/1121 [==============================] - 333s 298ms/step - loss: 1.9153\n",
            "\n",
            "Epoch 00009: loss improved from 1.96515 to 1.90866, saving model to model_weights/gigantic-improvement-ctd20-09-1.9087.hdf5\n",
            "Epoch 10/10\n",
            "1121/1121 [==============================] - 333s 297ms/step - loss: 1.8642\n",
            "\n",
            "Epoch 00010: loss improved from 1.90866 to 1.86254, saving model to model_weights/gigantic-improvement-ctd20-10-1.8625.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f850c0436d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1xQ82Jhrxxm"
      },
      "source": [
        "start = 10   #random row from the X array\r\n",
        "string_mapped = list(X[start])\r\n",
        "full_string = [n_to_char[value] for value in string_mapped]\r\n",
        "\r\n",
        "# generating characters\r\n",
        "for i in range(400):\r\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\r\n",
        "    x = x / float(len(characters))\r\n",
        "\r\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\r\n",
        "    seq = [n_to_char[value] for value in string_mapped]\r\n",
        "    full_string.append(n_to_char[pred_index])\r\n",
        "\r\n",
        "    string_mapped.append(pred_index)\r\n",
        "    string_mapped = string_mapped[1:len(string_mapped)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3j8dWfNwvRdu",
        "outputId": "0fa309ec-d0bf-451e-ec22-b2f5fc5a83de"
      },
      "source": [
        "result = \"\"\r\n",
        "for i in string_mapped:\r\n",
        "  result = result + characters[i]\r\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"sl eddk  ionliryend i tonele tonele ty ha \\n'ionlhry,' sa dna tonesy toc'\\n\\n'iorl tonel ons teresesege\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}